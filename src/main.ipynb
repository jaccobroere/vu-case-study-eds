{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VU Econometics and Data Science: Case Study\n",
    "```\n",
    "Author(s): Jacco Broere\n",
    "```\n",
    "\n",
    "\n",
    "### Setup\n",
    "- Setup config.ini file\n",
    "- Install necessary packages\n",
    "- Download and unpack data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import configparser\n",
    "import os\n",
    "# import sweetviz\n",
    "\n",
    "# helper functions\n",
    "from helpers.helper_functions import transform_data, add_actuals\n",
    "from helpers.helper_classes import AddFeatureNames\n",
    "\n",
    "# sklearn\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# feature_engine\n",
    "from feature_engine.selection import DropFeatures, DropConstantFeatures, DropDuplicateFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('src/config.ini')\n",
    "os.chdir(config['PATH']['ROOT_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "raw_train = pd.read_csv(config['PATH']['RAW_TRAIN_DATA'])\n",
    "raw_test = pd.read_csv(config['PATH']['RAW_TEST_DATA'])\n",
    "actuals = pd.read_csv(config['PATH']['ACTUALS'])\n",
    "\n",
    "# Read parameters\n",
    "SEED = config.getint('PARAMS', 'SEED')\n",
    "N_COMPONENTS = config.getint('PARAMS', 'N_COMPONENTS')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training set: 0\n",
      "Missing values in test set: 0\n",
      "Missing values in actuals: 0\n"
     ]
    }
   ],
   "source": [
    "# Check total missing values\n",
    "print(f\"Missing values in training set: {raw_train.isna().sum().sum()}\")\n",
    "print(f\"Missing values in test set: {raw_test.isna().sum().sum()}\")\n",
    "print(f\"Missing values in actuals: {actuals.isna().sum().sum()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to accesible format and add actuals\n",
    "train = transform_data(raw_train)\n",
    "train = add_actuals(train, actuals)\n",
    "test = transform_data(raw_test)\n",
    "test = add_actuals(test, actuals)\n",
    "\n",
    "# get target variable\n",
    "y_train = train[\"cancer\"]\n",
    "y_test = test[\"cancer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipe = Pipeline([\n",
    "    # Step 0:\n",
    "        # Drop constant and duplicate features\n",
    "        ('drop_features', DropFeatures(features_to_drop=[\"cancer\"])),\n",
    "        ('drop_constant', DropConstantFeatures(tol=0.98)),\n",
    "    \n",
    "    # Step 1:\n",
    "        # Apply scaling to data as it is a requirement for the variance maximization procedure of PCA\n",
    "        ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# X_train = preprocessing_pipe.fit_transform(train)\n",
    "# X_test = preprocessing_pipe.fit_transform(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and SparsePCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe = Pipeline([\n",
    "    # Step 0:\n",
    "        # Drop constant and duplicate features\n",
    "        ('drop_features', DropFeatures(features_to_drop=[\"cancer\"])),\n",
    "        ('drop_constant', DropConstantFeatures(tol=0.98)),\n",
    "    \n",
    "    # Step 1:\n",
    "        # Apply scaling to data as it is a requirement for the variance maximization procedure of PCA\n",
    "        ('scaler', StandardScaler()),\n",
    "    # Step 2:\n",
    "        # Apply PCA\n",
    "        ('pca', PCA(n_components=N_COMPONENTS, random_state=SEED)),\n",
    "    # Step 3:\n",
    "        # Add feature names\n",
    "        ('add_features_names', AddFeatureNames(prefix=\"cmpnt_\"))\n",
    "])\n",
    "\n",
    "spca_pipe = Pipeline([\n",
    "    # Step 0:\n",
    "        # Drop constant and duplicate features\n",
    "        ('drop_features', DropFeatures(features_to_drop=[\"cancer\"])),\n",
    "        ('drop_constant', DropConstantFeatures(tol=0.98)),\n",
    "    \n",
    "    # Step 1:\n",
    "        # Apply scaling to data as it is a requirement for the variance maximization procedure of PCA\n",
    "        ('scaler', StandardScaler()),\n",
    "    # Step 2:\n",
    "        # Apply SPCA\n",
    "        ('spca', SparsePCA(n_components=N_COMPONENTS, random_state=SEED, alpha=10, max_iter=50)),\n",
    "    # Step 3:\n",
    "        # Add feature names\n",
    "        ('add_features_names', AddFeatureNames(prefix=\"cmpnt_\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca_pipe.fit_transform(train)\n",
    "X_train_spca = spca_pipe.fit_transform(train)\n",
    "\n",
    "X_test_pca = pca_pipe.transform(test)\n",
    "X_test_spca = spca_pipe.transform(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting\n",
    "#### Implemented models\n",
    "- Logistic regression\n",
    "- LightGBM (Gradient Tree Boosting)\n",
    "- SVC (Support Vector Classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pca_pipe = Pipeline([\n",
    "    # Step 0:\n",
    "        # Drop constant and duplicate features\n",
    "        ('drop_features', DropFeatures(features_to_drop=[\"cancer\"])),\n",
    "        ('drop_constant', DropConstantFeatures(tol=0.98)),\n",
    "    \n",
    "    # Step 1:\n",
    "        # Apply scaling to data as it is a requirement for the variance maximization procedure of PCA\n",
    "        ('scaler', StandardScaler()),\n",
    "        \n",
    "    # Step 2:\n",
    "        # Apply PCA\n",
    "        ('pca', PCA(n_components=N_COMPONENTS, random_state=SEED)),\n",
    "    # Step 3:\n",
    "        # Add feature names\n",
    "        ('add_features_names', AddFeatureNames(prefix=\"cmpnt_\")),\n",
    "        \n",
    "    # Step 4:\n",
    "        # Apply logistic regression\n",
    "        ('logistic_regression', LogisticRegression(random_state=SEED))\n",
    "\n",
    "])\n",
    "\n",
    "lr_spca_pipe = Pipeline([\n",
    "    # Step 0:\n",
    "        # Drop constant and duplicate features\n",
    "        ('drop_features', DropFeatures(features_to_drop=[\"cancer\"])),\n",
    "        ('drop_constant', DropConstantFeatures(tol=0.98)),\n",
    "    \n",
    "    # Step 1:\n",
    "        # Apply scaling to data as it is a requirement for the variance maximization procedure of PCA\n",
    "        ('scaler', StandardScaler()),\n",
    "        \n",
    "    # Step 2:\n",
    "        # Apply SPCA\n",
    "        ('spca', SparsePCA(n_components=N_COMPONENTS, random_state=SEED, alpha=10, max_iter=100)),\n",
    "    # Step 3:\n",
    "        # Add feature names\n",
    "        ('add_features_names', AddFeatureNames(prefix=\"cmpnt_\")),\n",
    "        \n",
    "    # Step 4:\n",
    "        # Apply logistic regression\n",
    "        ('logistic_regression', LogisticRegression(random_state=SEED))\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with Logistic Regression and PCA:            0.7941176470588235\n",
      "Score with Logistic Regression and SPCA           : 0.5882352941176471\n"
     ]
    }
   ],
   "source": [
    "# Fit and score PCA with Logistic Regression\n",
    "lr_pca_pipe.fit(train, y_train)\n",
    "print(f\"{'Score with Logistic Regression and PCA:':<50} {lr_pca_pipe.score(test, y_test)}\")\n",
    "\n",
    "# Fit and score SPCA with Logistic Regression\n",
    "lr_spca_pipe.fit(train, y_train)\n",
    "print(f\"{'Score with Logistic Regression and SPCA:':<50} {lr_spca_pipe.score(test, y_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM (Gradient Tree Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_pca_pipe = Pipeline([\n",
    "    # Step 0:\n",
    "        # Drop constant and duplicate features\n",
    "        ('drop_features', DropFeatures(features_to_drop=[\"cancer\"])),\n",
    "        ('drop_constant', DropConstantFeatures(tol=0.98)),\n",
    "    \n",
    "    # Step 1:\n",
    "        # Apply scaling to data as it is a requirement for the variance maximization procedure of PCA\n",
    "        ('scaler', StandardScaler()),\n",
    "        \n",
    "    # Step 2:\n",
    "        # Apply PCA\n",
    "        ('pca', PCA(n_components=N_COMPONENTS, random_state=SEED)),\n",
    "    # Step 3:\n",
    "        # Add feature names\n",
    "        ('add_features_names', AddFeatureNames(prefix=\"cmpnt_\")),\n",
    "        \n",
    "    # Step 4:\n",
    "        # Apply logistic regression\n",
    "        ('lgbm_classifier', LGBMClassifier(random_state=SEED))\n",
    "\n",
    "])\n",
    "\n",
    "lgbm_spca_pipe = Pipeline([\n",
    "    # Step 0:\n",
    "        # Drop constant and duplicate features\n",
    "        ('drop_features', DropFeatures(features_to_drop=[\"cancer\"])),\n",
    "        ('drop_constant', DropConstantFeatures(tol=0.98)),\n",
    "    \n",
    "    # Step 1:\n",
    "        # Apply scaling to data as it is a requirement for the variance maximization procedure of PCA\n",
    "        ('scaler', StandardScaler()),\n",
    "        \n",
    "    # Step 2:\n",
    "        # Apply SPCA\n",
    "        ('spca', SparsePCA(n_components=N_COMPONENTS, random_state=SEED, alpha=10, max_iter=100)),\n",
    "    # Step 3:\n",
    "        # Add feature names\n",
    "        ('add_features_names', AddFeatureNames(prefix=\"cmpnt_\")),\n",
    "        \n",
    "    # Step 4:\n",
    "        # Apply logistic regression\n",
    "        ('lgbm_classifier', LGBMClassifier(random_state=SEED))\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with LGBM and PCA:                           0.5882352941176471\n",
      "Score with LGBM and SPCA                          : 0.5882352941176471\n"
     ]
    }
   ],
   "source": [
    "# Fit and score PCA with LGBM\n",
    "lgbm_pca_pipe.fit(train, y_train)\n",
    "print(f\"{'Score with LGBM and PCA:':<50} {lgbm_pca_pipe.score(test, y_test)}\")\n",
    "\n",
    "# Fit and score SPCA with LGBM\n",
    "lgbm_spca_pipe.fit(train, y_train)\n",
    "print(f\"{'Score with LGBM and SPCA:':<50} {lgbm_spca_pipe.score(test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92d3ca18b4289e5a59092b5e555528d322048c59e12087c1ad117261a9f3261c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
